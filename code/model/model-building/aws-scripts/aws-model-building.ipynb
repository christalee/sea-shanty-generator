{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "'''\n",
    "Adjust sequence length\n",
    "'''\n",
    "# Setting the sequence length\n",
    "seq_len = 50\n",
    "\n",
    "# Loading in Shanties lyrics corpus\n",
    "shanties = open('shanties_all.txt', encoding='utf-8').read()\n",
    "\n",
    "# Convering characters to integers\n",
    "\n",
    "# Creating a list of all unique characters\n",
    "chars_list = sorted(list(set(shanties)))\n",
    "\n",
    "# Creating a dictionary to map each unique character to a number\n",
    "chars_to_ints = dict((c, i) for i, c in enumerate(chars_list))\n",
    "\n",
    "# Checking length of corpus and unique characters\n",
    "len_shanties = len(shanties)\n",
    "n_chars = len(chars_list)\n",
    "\n",
    "print(f'Total length of corpus  :  {len_shanties}')\n",
    "print(f'Total unique characters :  {n_chars}')\n",
    "\n",
    "# Creating a list of patterns for the entire corpus\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(0, len_shanties - seq_len, 1):\n",
    "    seq_in = shanties[i:i + seq_len]\n",
    "    seq_out = shanties[i + seq_len]\n",
    "    X_data.append([chars_to_ints[char] for char in seq_in])\n",
    "    y_data.append(chars_to_ints[seq_out])\n",
    "\n",
    "total_patterns = len(X_data)\n",
    "print(f'Total number of {seq_len} character lenght patters: {total_patterns}')\n",
    "\n",
    "# Reshaping Data for use in LSTM networks\n",
    "X = np.reshape(X_data, (total_patterns, seq_len, 1))\n",
    "\n",
    "# Normalzing X data\n",
    "X = X / float(n_chars)\n",
    "\n",
    "# One hot encode to the output variable\n",
    "y = np_utils.to_categorical(y_data)\n",
    "\n",
    "# Creating directory to store model weights\n",
    "os.mkdir(f'{seq_len}-char-model-weights-test')\n",
    "\n",
    "# Creating a checkpoint to find best weights\n",
    "checkpoint_name = './' + str(seq_len) + '-char-model-weights/' + str(seq_len) + '-char-sequence/' + str(seq_len) + '-char-seq-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Defining LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compiling model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
    "\n",
    "# Saving Model\n",
    "model.save(f'{seq_len}-char-seq-shanty-writer.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mode-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
